{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R1T20UtPgEIE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Bottleneck unit testing available.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "feY03JQnNLDj"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ErhCFUOSgkWt"
   },
   "outputs": [],
   "source": [
    "with open('multilabel_train_data.txt') as f:\n",
    "    data_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "30vzhnIkg39N"
   },
   "outputs": [],
   "source": [
    "#data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylbVBjpzg5e5",
    "outputId": "96538235-384f-4b2e-b9fb-f2c13671f29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103]\n",
      "labels are: [10  1 12  0  7  8 13  6  3 11  9  2  4  5]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "features=[]\n",
    "\n",
    "for line in data_list:\n",
    "  line_list=line.split(' ')\n",
    "  labels.append(set(line_list[0].split(',')))\n",
    "  for j in line_list[1:-1]:\n",
    "    features.append(int(j.split(':')[0]))\n",
    "    \n",
    "features=np.array(features)\n",
    "label_set=set([])\n",
    "for i in labels:\n",
    "  label_set=set.union(label_set,i)\n",
    "label_arr=list(label_set)\n",
    "labels_arr=np.array([int(label_arr[i]) for i in range(len(label_arr))])\n",
    "\n",
    "features_arr=np.unique(features)\n",
    "print('features are:',features_arr) #feature space \n",
    "print('labels are:',labels_arr) # label space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQtAOe7Di1zG",
    "outputId": "548afe09-826a-4caa-a90a-37df3fa02ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Array: array of labels:\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "\n",
      " X array: array of features:\n",
      "\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103]\n"
     ]
    }
   ],
   "source": [
    "print('y Array: array of labels:\\n')\n",
    "labels_arr.sort()\n",
    "print(labels_arr)\n",
    "print('\\n X array: array of features:\\n')\n",
    "print(features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbdP7qsFxxV5",
    "outputId": "29d8dbd3-705f-4de8-eca6-14884a84beac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in dataset: 14 \n",
      "\n",
      "Number of features in dataset: 103 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of classes in dataset:',len(labels_arr),'\\n')\n",
    "print('Number of features in dataset:',len(features_arr),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGJTSTB25d-J",
    "outputId": "9cdf273d-2954-41a8-8bcc-04ab26af9944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 1500\n"
     ]
    }
   ],
   "source": [
    "# number of data points\n",
    "n=len(data_list)\n",
    "print('number of data points:',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CFpRZZ15iYf",
    "outputId": "22890de1-3da6-449e-f570-dfb05dcb0b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 103\n"
     ]
    }
   ],
   "source": [
    "#number of features\n",
    "d=len(features_arr)\n",
    "print('number of features:',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DHzDCt95kRE",
    "outputId": "f97cf335-7c52-42d9-8a8c-d2601f13697e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 103)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=np.array([0.0 for i in range(n*(d))]).reshape(n,d)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15ByhcBB5nZo",
    "outputId": "16ef2653-603c-42d9-e362-747c206053a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for i in range(len(data_list)):\n",
    "  line=data_list[i]\n",
    "  line_list=line.split(' ')\n",
    "  labels.append(set(line_list[0].split(',')))\n",
    "  #print(i)\n",
    "  for j in line_list[1:-1]:\n",
    "    a=j.split(':')\n",
    "    #print(a,int(a[0])-1,a[1])\n",
    "    array[i,int(a[0])-1]=a[1]\n",
    "len(array) #value of data points corresponding to feature space   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Osd32eOx-8UD",
    "outputId": "ed7d40aa-aea9-4c0c-9b7f-57598b58fb87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011858</td>\n",
       "      <td>-0.061636</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>0.021137</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>-0.044525</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>-0.053591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017531</td>\n",
       "      <td>-0.032930</td>\n",
       "      <td>-0.030487</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>-0.002603</td>\n",
       "      <td>0.049288</td>\n",
       "      <td>-0.046913</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.115765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018183</td>\n",
       "      <td>-0.045645</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.118108</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.060929</td>\n",
       "      <td>0.284897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033050</td>\n",
       "      <td>-0.159394</td>\n",
       "      <td>0.125488</td>\n",
       "      <td>-0.149774</td>\n",
       "      <td>-0.148373</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.079903</td>\n",
       "      <td>0.065097</td>\n",
       "      <td>-0.165039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074145</td>\n",
       "      <td>-0.044487</td>\n",
       "      <td>0.048191</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>0.048671</td>\n",
       "      <td>-0.038404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099730</td>\n",
       "      <td>0.145222</td>\n",
       "      <td>-0.102574</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.135860</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>-0.104915</td>\n",
       "      <td>-0.085630</td>\n",
       "      <td>0.174316</td>\n",
       "      <td>-0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044432</td>\n",
       "      <td>-0.007842</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.154859</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053067</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>-0.056184</td>\n",
       "      <td>-0.056243</td>\n",
       "      <td>-0.049476</td>\n",
       "      <td>-0.056670</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>0.086152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051570</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.155760</td>\n",
       "      <td>0.048347</td>\n",
       "      <td>-0.031283</td>\n",
       "      <td>0.133782</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>-0.047716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107924</td>\n",
       "      <td>-0.086031</td>\n",
       "      <td>-0.098521</td>\n",
       "      <td>-0.086622</td>\n",
       "      <td>-0.091729</td>\n",
       "      <td>0.210266</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.140546</td>\n",
       "      <td>0.182154</td>\n",
       "      <td>-0.101904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.116754</td>\n",
       "      <td>-0.017859</td>\n",
       "      <td>-0.051158</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.121055</td>\n",
       "      <td>-0.099059</td>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057323</td>\n",
       "      <td>-0.125292</td>\n",
       "      <td>0.177598</td>\n",
       "      <td>-0.127144</td>\n",
       "      <td>-0.126141</td>\n",
       "      <td>0.046453</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>-0.039471</td>\n",
       "      <td>-0.016383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.085234</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.091247</td>\n",
       "      <td>0.239547</td>\n",
       "      <td>0.140304</td>\n",
       "      <td>0.166942</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.126451</td>\n",
       "      <td>0.135792</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046560</td>\n",
       "      <td>0.071134</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>-0.040123</td>\n",
       "      <td>-0.056090</td>\n",
       "      <td>-0.052238</td>\n",
       "      <td>-0.020390</td>\n",
       "      <td>0.085778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.074028</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>-0.041893</td>\n",
       "      <td>0.030209</td>\n",
       "      <td>-0.061547</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>-0.022888</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.019675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064247</td>\n",
       "      <td>0.158470</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>0.188122</td>\n",
       "      <td>0.182037</td>\n",
       "      <td>-0.056295</td>\n",
       "      <td>-0.075996</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.039109</td>\n",
       "      <td>0.019538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.058193</td>\n",
       "      <td>0.502205</td>\n",
       "      <td>-0.081411</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.086514</td>\n",
       "      <td>-0.005469</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>-0.044017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054680</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.050850</td>\n",
       "      <td>-0.038719</td>\n",
       "      <td>0.178632</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>-0.062065</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>0.498207</td>\n",
       "      <td>-0.072177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-0.128468</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>0.039187</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>-0.006359</td>\n",
       "      <td>-0.049659</td>\n",
       "      <td>-0.147537</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020853</td>\n",
       "      <td>-0.044238</td>\n",
       "      <td>-0.065766</td>\n",
       "      <td>-0.062276</td>\n",
       "      <td>-0.069627</td>\n",
       "      <td>-0.010034</td>\n",
       "      <td>0.086035</td>\n",
       "      <td>-0.062728</td>\n",
       "      <td>-0.048271</td>\n",
       "      <td>-0.124432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2         3         4         5         6         7    \\\n",
       "0    -0.011858 -0.061636  0.054392  0.021137  0.089374  0.101825 -0.044525   \n",
       "1    -0.018183 -0.045645  0.002001  0.066467 -0.118108  0.105399  0.004705   \n",
       "2     0.074145 -0.044487  0.048191 -0.006614  0.042746  0.025960  0.021290   \n",
       "3     0.044432 -0.007842  0.018440  0.036639  0.243409  0.154859  0.128655   \n",
       "4     0.051570 -0.016571  0.014702  0.024296  0.155760  0.048347 -0.031283   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495 -0.116754 -0.017859 -0.051158  0.015938  0.015013  0.121055 -0.099059   \n",
       "1496  0.085234  0.049690  0.134850  0.091247  0.239547  0.140304  0.166942   \n",
       "1497  0.074028  0.014501 -0.041893  0.030209 -0.061547  0.022142 -0.022888   \n",
       "1498 -0.058193  0.502205 -0.081411 -0.033651  0.001250  0.086514 -0.005469   \n",
       "1499 -0.128468 -0.003593  0.039187  0.032379 -0.006359 -0.049659 -0.147537   \n",
       "\n",
       "           8         9         10   ...       94        95        96   \\\n",
       "0     0.222330  0.015575 -0.053591  ... -0.017531 -0.032930 -0.030487   \n",
       "1     0.046744 -0.060929  0.284897  ...  0.033050 -0.159394  0.125488   \n",
       "2     0.041714  0.048671 -0.038404  ... -0.099730  0.145222 -0.102574   \n",
       "3     0.122329  0.004179  0.106768  ... -0.053067  0.011885 -0.052333   \n",
       "4     0.133782  0.085081 -0.047716  ... -0.107924 -0.086031 -0.098521   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1495  0.023402  0.005873  0.019754  ...  0.057323 -0.125292  0.177598   \n",
       "1496  0.050181  0.126451  0.135792  ... -0.046560  0.071134 -0.041950   \n",
       "1497  0.023375  0.073267  0.019675  ... -0.064247  0.158470 -0.061079   \n",
       "1498  0.033748  0.007124 -0.044017  ... -0.054680 -0.000601 -0.050850   \n",
       "1499 -0.092815  0.050572  0.001716  ... -0.020853 -0.044238 -0.065766   \n",
       "\n",
       "           97        98        99        100       101       102       103  \n",
       "0     0.009813 -0.002603  0.049288 -0.046913 -0.000844  0.004983  0.115765  \n",
       "1    -0.149774 -0.148373  0.135500  0.047781  0.079903  0.065097 -0.165039  \n",
       "2     0.143100  0.135860 -0.098214 -0.104915 -0.085630  0.174316 -0.022570  \n",
       "3    -0.056184 -0.056243 -0.049476 -0.056670  0.120910 -0.028053  0.086152  \n",
       "4    -0.086622 -0.091729  0.210266  0.110243  0.140546  0.182154 -0.101904  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1495 -0.127144 -0.126141  0.046453 -0.070558  0.138768 -0.039471 -0.016383  \n",
       "1496  0.076210  0.077935 -0.040123 -0.056090 -0.052238 -0.020390  0.085778  \n",
       "1497  0.188122  0.182037 -0.056295 -0.075996 -0.072424 -0.039109  0.019538  \n",
       "1498 -0.038719  0.178632  0.151786 -0.062065 -0.017584  0.498207 -0.072177  \n",
       "1499 -0.062276 -0.069627 -0.010034  0.086035 -0.062728 -0.048271 -0.124432  \n",
       "\n",
       "[1500 rows x 103 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame(array,columns=features_arr)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6iv-aa2Mbmjr"
   },
   "outputs": [],
   "source": [
    "data['labels']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "aIwycuVWfHyw",
    "outputId": "94ce8750-65a6-4c62-c9e7-d48ef8d537e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011858</td>\n",
       "      <td>-0.061636</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>0.021137</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>-0.044525</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>-0.053591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032930</td>\n",
       "      <td>-0.030487</td>\n",
       "      <td>0.009813</td>\n",
       "      <td>-0.002603</td>\n",
       "      <td>0.049288</td>\n",
       "      <td>-0.046913</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.115765</td>\n",
       "      <td>{11, 6, 7, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018183</td>\n",
       "      <td>-0.045645</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.118108</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.060929</td>\n",
       "      <td>0.284897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159394</td>\n",
       "      <td>0.125488</td>\n",
       "      <td>-0.149774</td>\n",
       "      <td>-0.148373</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.079903</td>\n",
       "      <td>0.065097</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>{11, 4, 3, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074145</td>\n",
       "      <td>-0.044487</td>\n",
       "      <td>0.048191</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>0.048671</td>\n",
       "      <td>-0.038404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145222</td>\n",
       "      <td>-0.102574</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.135860</td>\n",
       "      <td>-0.098214</td>\n",
       "      <td>-0.104915</td>\n",
       "      <td>-0.085630</td>\n",
       "      <td>0.174316</td>\n",
       "      <td>-0.022570</td>\n",
       "      <td>{10, 12, 7, 6, 11, 4, 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044432</td>\n",
       "      <td>-0.007842</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.154859</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>-0.056184</td>\n",
       "      <td>-0.056243</td>\n",
       "      <td>-0.049476</td>\n",
       "      <td>-0.056670</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>0.086152</td>\n",
       "      <td>{10, 1, 2, 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051570</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.155760</td>\n",
       "      <td>0.048347</td>\n",
       "      <td>-0.031283</td>\n",
       "      <td>0.133782</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>-0.047716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086031</td>\n",
       "      <td>-0.098521</td>\n",
       "      <td>-0.086622</td>\n",
       "      <td>-0.091729</td>\n",
       "      <td>0.210266</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.140546</td>\n",
       "      <td>0.182154</td>\n",
       "      <td>-0.101904</td>\n",
       "      <td>{11, 1, 2, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.116754</td>\n",
       "      <td>-0.017859</td>\n",
       "      <td>-0.051158</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.121055</td>\n",
       "      <td>-0.099059</td>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125292</td>\n",
       "      <td>0.177598</td>\n",
       "      <td>-0.127144</td>\n",
       "      <td>-0.126141</td>\n",
       "      <td>0.046453</td>\n",
       "      <td>-0.070558</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>-0.039471</td>\n",
       "      <td>-0.016383</td>\n",
       "      <td>{11, 1, 0, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.085234</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.091247</td>\n",
       "      <td>0.239547</td>\n",
       "      <td>0.140304</td>\n",
       "      <td>0.166942</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.126451</td>\n",
       "      <td>0.135792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071134</td>\n",
       "      <td>-0.041950</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>-0.040123</td>\n",
       "      <td>-0.056090</td>\n",
       "      <td>-0.052238</td>\n",
       "      <td>-0.020390</td>\n",
       "      <td>0.085778</td>\n",
       "      <td>{11, 4, 3, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.074028</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>-0.041893</td>\n",
       "      <td>0.030209</td>\n",
       "      <td>-0.061547</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>-0.022888</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.019675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158470</td>\n",
       "      <td>-0.061079</td>\n",
       "      <td>0.188122</td>\n",
       "      <td>0.182037</td>\n",
       "      <td>-0.056295</td>\n",
       "      <td>-0.075996</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.039109</td>\n",
       "      <td>0.019538</td>\n",
       "      <td>{11, 4, 3, 12}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.058193</td>\n",
       "      <td>0.502205</td>\n",
       "      <td>-0.081411</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.086514</td>\n",
       "      <td>-0.005469</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>-0.044017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>-0.050850</td>\n",
       "      <td>-0.038719</td>\n",
       "      <td>0.178632</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>-0.062065</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>0.498207</td>\n",
       "      <td>-0.072177</td>\n",
       "      <td>{8, 7}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-0.128468</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>0.039187</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>-0.006359</td>\n",
       "      <td>-0.049659</td>\n",
       "      <td>-0.147537</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044238</td>\n",
       "      <td>-0.065766</td>\n",
       "      <td>-0.062276</td>\n",
       "      <td>-0.069627</td>\n",
       "      <td>-0.010034</td>\n",
       "      <td>0.086035</td>\n",
       "      <td>-0.062728</td>\n",
       "      <td>-0.048271</td>\n",
       "      <td>-0.124432</td>\n",
       "      <td>{10, 11, 9, 12}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "0    -0.011858 -0.061636  0.054392  0.021137  0.089374  0.101825 -0.044525   \n",
       "1    -0.018183 -0.045645  0.002001  0.066467 -0.118108  0.105399  0.004705   \n",
       "2     0.074145 -0.044487  0.048191 -0.006614  0.042746  0.025960  0.021290   \n",
       "3     0.044432 -0.007842  0.018440  0.036639  0.243409  0.154859  0.128655   \n",
       "4     0.051570 -0.016571  0.014702  0.024296  0.155760  0.048347 -0.031283   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495 -0.116754 -0.017859 -0.051158  0.015938  0.015013  0.121055 -0.099059   \n",
       "1496  0.085234  0.049690  0.134850  0.091247  0.239547  0.140304  0.166942   \n",
       "1497  0.074028  0.014501 -0.041893  0.030209 -0.061547  0.022142 -0.022888   \n",
       "1498 -0.058193  0.502205 -0.081411 -0.033651  0.001250  0.086514 -0.005469   \n",
       "1499 -0.128468 -0.003593  0.039187  0.032379 -0.006359 -0.049659 -0.147537   \n",
       "\n",
       "             8         9        10  ...        95        96        97  \\\n",
       "0     0.222330  0.015575 -0.053591  ... -0.032930 -0.030487  0.009813   \n",
       "1     0.046744 -0.060929  0.284897  ... -0.159394  0.125488 -0.149774   \n",
       "2     0.041714  0.048671 -0.038404  ...  0.145222 -0.102574  0.143100   \n",
       "3     0.122329  0.004179  0.106768  ...  0.011885 -0.052333 -0.056184   \n",
       "4     0.133782  0.085081 -0.047716  ... -0.086031 -0.098521 -0.086622   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1495  0.023402  0.005873  0.019754  ... -0.125292  0.177598 -0.127144   \n",
       "1496  0.050181  0.126451  0.135792  ...  0.071134 -0.041950  0.076210   \n",
       "1497  0.023375  0.073267  0.019675  ...  0.158470 -0.061079  0.188122   \n",
       "1498  0.033748  0.007124 -0.044017  ... -0.000601 -0.050850 -0.038719   \n",
       "1499 -0.092815  0.050572  0.001716  ... -0.044238 -0.065766 -0.062276   \n",
       "\n",
       "            98        99       100       101       102       103  \\\n",
       "0    -0.002603  0.049288 -0.046913 -0.000844  0.004983  0.115765   \n",
       "1    -0.148373  0.135500  0.047781  0.079903  0.065097 -0.165039   \n",
       "2     0.135860 -0.098214 -0.104915 -0.085630  0.174316 -0.022570   \n",
       "3    -0.056243 -0.049476 -0.056670  0.120910 -0.028053  0.086152   \n",
       "4    -0.091729  0.210266  0.110243  0.140546  0.182154 -0.101904   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "1495 -0.126141  0.046453 -0.070558  0.138768 -0.039471 -0.016383   \n",
       "1496  0.077935 -0.040123 -0.056090 -0.052238 -0.020390  0.085778   \n",
       "1497  0.182037 -0.056295 -0.075996 -0.072424 -0.039109  0.019538   \n",
       "1498  0.178632  0.151786 -0.062065 -0.017584  0.498207 -0.072177   \n",
       "1499 -0.069627 -0.010034  0.086035 -0.062728 -0.048271 -0.124432   \n",
       "\n",
       "                        labels  \n",
       "0               {11, 6, 7, 12}  \n",
       "1               {11, 4, 3, 12}  \n",
       "2     {10, 12, 7, 6, 11, 4, 5}  \n",
       "3                {10, 1, 2, 9}  \n",
       "4               {11, 1, 2, 12}  \n",
       "...                        ...  \n",
       "1495            {11, 1, 0, 12}  \n",
       "1496            {11, 4, 3, 12}  \n",
       "1497            {11, 4, 3, 12}  \n",
       "1498                    {8, 7}  \n",
       "1499           {10, 11, 9, 12}  \n",
       "\n",
       "[1500 rows x 104 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mvvu4c-sGusG",
    "outputId": "8e28f4be-7034-4c2a-d3da-17372d918364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CpQM3IaGTLi",
    "outputId": "770d4969-3de8-4d6d-b6b7-14e3d8f2baf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct classes 14\n"
     ]
    }
   ],
   "source": [
    "# number of labels\n",
    "k=len(labels_arr)\n",
    "print('number of distinct classes',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQuBVUvSHPFO",
    "outputId": "d97002a1-fad3-4cf5-f22f-1aeed57d7f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 14)\n"
     ]
    }
   ],
   "source": [
    "label_matrix=np.array([0 for i in range(n*k)]).reshape((n,k))\n",
    "for i in range(n):\n",
    "  for l in range(len(labels_arr)):\n",
    "    if str(labels_arr[l]) in data.labels[i]:\n",
    "      label_matrix[i][l]=1\n",
    "    else:\n",
    "      label_matrix[i][l]=-1\n",
    "print(label_matrix.shape)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnzCeBqp-PjS",
    "outputId": "0c572e45-9638-40d3-9811-4911dcb9c8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kWFSK3K4f-pn"
   },
   "outputs": [],
   "source": [
    "def lab(x,i):\n",
    "  if str(i) in x:\n",
    "    return 1\n",
    "  else:\n",
    "    return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MizZ51x-c7C",
    "outputId": "51eb0fc6-fabd-427e-ca3f-a9bae90867b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d8WV_RdSgNB9"
   },
   "outputs": [],
   "source": [
    "for i in labels_arr:\n",
    "  data[f'labels_{i}']=data['labels'].apply(lambda x: lab(x,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "viwkGcBggSZv",
    "outputId": "7a27c83a-71ba-4653-cd11-8f1932fffb35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>labels_4</th>\n",
       "      <th>labels_5</th>\n",
       "      <th>labels_6</th>\n",
       "      <th>labels_7</th>\n",
       "      <th>labels_8</th>\n",
       "      <th>labels_9</th>\n",
       "      <th>labels_10</th>\n",
       "      <th>labels_11</th>\n",
       "      <th>labels_12</th>\n",
       "      <th>labels_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011858</td>\n",
       "      <td>-0.061636</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>0.021137</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>-0.044525</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>-0.053591</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018183</td>\n",
       "      <td>-0.045645</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.118108</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.060929</td>\n",
       "      <td>0.284897</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074145</td>\n",
       "      <td>-0.044487</td>\n",
       "      <td>0.048191</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>0.048671</td>\n",
       "      <td>-0.038404</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044432</td>\n",
       "      <td>-0.007842</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.154859</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051570</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.155760</td>\n",
       "      <td>0.048347</td>\n",
       "      <td>-0.031283</td>\n",
       "      <td>0.133782</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>-0.047716</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-0.116754</td>\n",
       "      <td>-0.017859</td>\n",
       "      <td>-0.051158</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.121055</td>\n",
       "      <td>-0.099059</td>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.085234</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.091247</td>\n",
       "      <td>0.239547</td>\n",
       "      <td>0.140304</td>\n",
       "      <td>0.166942</td>\n",
       "      <td>0.050181</td>\n",
       "      <td>0.126451</td>\n",
       "      <td>0.135792</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.074028</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>-0.041893</td>\n",
       "      <td>0.030209</td>\n",
       "      <td>-0.061547</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>-0.022888</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.073267</td>\n",
       "      <td>0.019675</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>-0.058193</td>\n",
       "      <td>0.502205</td>\n",
       "      <td>-0.081411</td>\n",
       "      <td>-0.033651</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.086514</td>\n",
       "      <td>-0.005469</td>\n",
       "      <td>0.033748</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>-0.044017</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>-0.128468</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>0.039187</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>-0.006359</td>\n",
       "      <td>-0.049659</td>\n",
       "      <td>-0.147537</td>\n",
       "      <td>-0.092815</td>\n",
       "      <td>0.050572</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "0    -0.011858 -0.061636  0.054392  0.021137  0.089374  0.101825 -0.044525   \n",
       "1    -0.018183 -0.045645  0.002001  0.066467 -0.118108  0.105399  0.004705   \n",
       "2     0.074145 -0.044487  0.048191 -0.006614  0.042746  0.025960  0.021290   \n",
       "3     0.044432 -0.007842  0.018440  0.036639  0.243409  0.154859  0.128655   \n",
       "4     0.051570 -0.016571  0.014702  0.024296  0.155760  0.048347 -0.031283   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1495 -0.116754 -0.017859 -0.051158  0.015938  0.015013  0.121055 -0.099059   \n",
       "1496  0.085234  0.049690  0.134850  0.091247  0.239547  0.140304  0.166942   \n",
       "1497  0.074028  0.014501 -0.041893  0.030209 -0.061547  0.022142 -0.022888   \n",
       "1498 -0.058193  0.502205 -0.081411 -0.033651  0.001250  0.086514 -0.005469   \n",
       "1499 -0.128468 -0.003593  0.039187  0.032379 -0.006359 -0.049659 -0.147537   \n",
       "\n",
       "             8         9        10  ...  labels_4  labels_5  labels_6  \\\n",
       "0     0.222330  0.015575 -0.053591  ...        -1        -1         1   \n",
       "1     0.046744 -0.060929  0.284897  ...         1        -1        -1   \n",
       "2     0.041714  0.048671 -0.038404  ...         1         1         1   \n",
       "3     0.122329  0.004179  0.106768  ...        -1        -1        -1   \n",
       "4     0.133782  0.085081 -0.047716  ...        -1        -1        -1   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1495  0.023402  0.005873  0.019754  ...        -1        -1        -1   \n",
       "1496  0.050181  0.126451  0.135792  ...         1        -1        -1   \n",
       "1497  0.023375  0.073267  0.019675  ...         1        -1        -1   \n",
       "1498  0.033748  0.007124 -0.044017  ...        -1        -1        -1   \n",
       "1499 -0.092815  0.050572  0.001716  ...        -1        -1        -1   \n",
       "\n",
       "      labels_7  labels_8  labels_9  labels_10  labels_11  labels_12  labels_13  \n",
       "0            1        -1        -1         -1          1          1         -1  \n",
       "1           -1        -1        -1         -1          1          1         -1  \n",
       "2            1        -1        -1          1          1          1         -1  \n",
       "3           -1        -1         1          1         -1         -1         -1  \n",
       "4           -1        -1        -1         -1          1          1         -1  \n",
       "...        ...       ...       ...        ...        ...        ...        ...  \n",
       "1495        -1        -1        -1         -1          1          1         -1  \n",
       "1496        -1        -1        -1         -1          1          1         -1  \n",
       "1497        -1        -1        -1         -1          1          1         -1  \n",
       "1498         1         1        -1         -1         -1         -1         -1  \n",
       "1499        -1        -1         1          1          1          1         -1  \n",
       "\n",
       "[1500 rows x 118 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4lCN6jUcF8F"
   },
   "source": [
    "$\\huge \\text{Adaboost function from scratch for binary classifier:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "diF-HX1K5b_B"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6Pi-FJTBA4Z-"
   },
   "outputs": [],
   "source": [
    "def Adaboost_fit(data_X,data_y,rounds):\n",
    "  weight_all=[]\n",
    "  epsilon_all=[]\n",
    "  alpha_all=[]\n",
    "  n=len(data_X) #number of data points\n",
    "  d=data_X.shape[1] #dimension of data\n",
    "  #initialization of weight vector\n",
    "  w=np.array([1/n for i in range(n)])\n",
    "  weight_all.append(w)\n",
    "  # rounds\n",
    "  T=rounds \n",
    "  # we need to store classifier\n",
    "  classifiers=[]\n",
    "  \n",
    "  for round in range(T):\n",
    "    weak_classifier=DecisionTreeClassifier(max_depth=5)\n",
    "    weak_classifier.fit(data_X,data_y,sample_weight=w)\n",
    "    classifiers.append(weak_classifier)\n",
    "    #prediction\n",
    "    y_pred=weak_classifier.predict(data_X)\n",
    "    \n",
    "    #epsilon value\n",
    "    epsilon=np.sum(w*(y_pred!=data_y))\n",
    "    epsilon_all.append(epsilon)\n",
    "    #alpha values\n",
    "    alpha=(1/2)*np.log((1-epsilon)/epsilon)\n",
    "    alpha_all.append(alpha)\n",
    "    # updatation the weight vector\n",
    "    w=w*np.exp(-1*alpha*y_pred*data_y)\n",
    "    # normalization of weight vector\n",
    "    w=w/np.sum(w)\n",
    "    # storing the w\n",
    "    weight_all.append(w)\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "  return alpha_all,weight_all,epsilon_all,classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJ_Xrx0cVLB"
   },
   "source": [
    "$\\text{We use Decision tree classifier as a base classifier.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VB4IzGKcd5I"
   },
   "source": [
    "$\\huge \\text{ prediction function}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8C-ktgCi5lM8"
   },
   "outputs": [],
   "source": [
    "def AdaBoost_pred(data_X,alpha_all,classifiers):\n",
    "  T=len(alpha_all)\n",
    "  n=len(data_X)\n",
    "  predictions=[]\n",
    "  for i in range(T):\n",
    "    predictions.append(classifiers[i].predict(data_X))\n",
    "  pred=np.zeros(n)  \n",
    "  for i in range(T):\n",
    "    pred=pred+alpha_all[i]*predictions[i]\n",
    "    \n",
    "  return np.sign(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94BuOOcNcmho"
   },
   "source": [
    "$\\huge \\text{Accuracy function for binary classifier:}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5S8jrL755lt2"
   },
   "outputs": [],
   "source": [
    "def Accuracy_adaboost(prediction,actual_label):\n",
    "  return np.sum(prediction==actual_label)/len(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_v6xkE1cwvA"
   },
   "source": [
    "$\\text{We first apply our binary classifier first our label 0. Here we consider the label of data point is +1 is label 0 is present in its label set otherwise -1. }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X=data[data.columns[:-15]]\n",
    "data_y=data[data.columns[-14]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X,data_y,test_size=0.2,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6FUFON6Q8HXz"
   },
   "outputs": [],
   "source": [
    "alphas,weights,epsilons,classifiers=Adaboost_fit(X_train,y_train,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d3LjdtIDQ6g",
    "outputId": "c909d653-01c7-44fc-d469-12920f1107ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# prediction over train data\n",
    "y_pred=AdaBoost_pred(X_train,alphas,classifiers)\n",
    "accuracy=Accuracy_adaboost(y_pred,np.array(y_train))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vwQoF1WiCwkY",
    "outputId": "68fe6230-a805-4529-fa00-1f6c4691e12d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "# prediction over test data\n",
    "y_pred=AdaBoost_pred(X_test,alphas,classifiers)\n",
    "accuracy=Accuracy_adaboost(y_pred,np.array(y_test))\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yq5M9FESt8pp"
   },
   "source": [
    "$\\text{As we saw that our model is giving 100% accuracy over training data and 79% accuracy of test data.}$\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhlcbg20uAaO"
   },
   "source": [
    "$\\huge \\text{Accuracy function (defined in Question)}$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eDFe2IEqGh6x"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_prediction,y_actual):\n",
    "  n=len(y_actual)\n",
    "  sum=0\n",
    "  for i in range(n):\n",
    "    sum+=len(y_prediction[i].intersection(y_actual[i]))/len(y_prediction[i].union(y_actual[i]))\n",
    "\n",
    "  return (1/n)*sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xT0ZgKmuDj2"
   },
   "source": [
    "$\\huge \\text{F1 score function (defined in Question)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lHJaLIaVHk1a"
   },
   "outputs": [],
   "source": [
    "def F1_score(y_prediction,y_actual):\n",
    "  n=len(y_actual)\n",
    "  sum=0\n",
    "  for i in range(n):\n",
    "    sum+=len(y_prediction[i].intersection(y_actual[i]))/(len(y_prediction[i])+len(y_actual[i]))\n",
    "  return (1/n)*sum    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HavtbGfSNImF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4SYnus0YI9rB"
   },
   "outputs": [],
   "source": [
    "seed_values=[24,34,44,54,65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "diQNqn0GDDFL"
   },
   "outputs": [],
   "source": [
    "def multitask_adaboost(data,k_values):\n",
    "  start_1=timer()\n",
    "  f1_score_k=[]\n",
    "  accuracy_k=[]\n",
    "  for k in k_values:\n",
    "    start_2=timer()\n",
    "    print('k value is :',k,'\\n***********************************\\n')\n",
    "    data_X=data[data.columns[:-15]]\n",
    "    data_y=data[data.columns[-15:]]\n",
    "    f1_list=[]\n",
    "    acc_list=[]\n",
    "    for seed in seed_values:\n",
    "      print('when seed is :',seed,'\\n***************************************\\n')\n",
    "      X_train, X_test, y_train, y_test = train_test_split(data_X,data_y,test_size=0.2,random_state=seed)\n",
    "      pred=[set([]) for i in range(len(X_test))]\n",
    "      for i in range(len(labels_arr)):\n",
    "        y=y_train[f'labels_{i}']\n",
    "        \n",
    "        # cross validation\n",
    "        alphas,weights,epsilons,classifiers=Adaboost_fit(X_train,y,k)\n",
    "        # prediction over test data\n",
    "        y_pred=AdaBoost_pred(X_test,alphas,classifiers)\n",
    "        \n",
    "        #print(np.sum(y_pred))\n",
    "        for m in range(len(y_pred)):\n",
    "          if y_pred[m]==1:\n",
    "            \n",
    "            pred[m]=pred[m].union({str(i)})\n",
    "      #print(pred)\n",
    "      f1_score=F1_score(pred,np.array(y_test[y_test.columns[0]]))\n",
    "      acc=accuracy(pred,np.array(y_test[y_test.columns[0]]))\n",
    "      print('F1 score for test data with seed value:',seed,'is:',f1_score,'accuracy is:',acc)\n",
    "      # storing f1 score values\n",
    "      f1_list.append(f1_score)\n",
    "      acc_list.append(acc)\n",
    "    end_2=timer()\n",
    "    print(f'time taken for k={k} value:',end_2-start_2)\n",
    "    print(f'mean f1 for k={k}:',np.mean(f1_list),'mean accuracy is:',np.mean(acc_list))\n",
    "    f1_score_k.append(np.mean(f1_list))  \n",
    "    accuracy_k.append(np.mean(acc_list))\n",
    "  end_1=timer()\n",
    "  print('total time taken:',end_1-start_1)\n",
    "  return k_values[np.argmax(np.array(f1_score_k))],k_values[np.argmax(np.array(accuracy_k))],np.array(f1_score_k),np.array(accuracy_k)  # best k value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "fZ_eWUqlPe12",
    "outputId": "e91f82a9-1c78-4f89-a645-ea7467bd4b97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>labels_4</th>\n",
       "      <th>labels_5</th>\n",
       "      <th>labels_6</th>\n",
       "      <th>labels_7</th>\n",
       "      <th>labels_8</th>\n",
       "      <th>labels_9</th>\n",
       "      <th>labels_10</th>\n",
       "      <th>labels_11</th>\n",
       "      <th>labels_12</th>\n",
       "      <th>labels_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011858</td>\n",
       "      <td>-0.061636</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>0.021137</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.101825</td>\n",
       "      <td>-0.044525</td>\n",
       "      <td>0.222330</td>\n",
       "      <td>0.015575</td>\n",
       "      <td>-0.053591</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018183</td>\n",
       "      <td>-0.045645</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>-0.118108</td>\n",
       "      <td>0.105399</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.060929</td>\n",
       "      <td>0.284897</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.074145</td>\n",
       "      <td>-0.044487</td>\n",
       "      <td>0.048191</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.025960</td>\n",
       "      <td>0.021290</td>\n",
       "      <td>0.041714</td>\n",
       "      <td>0.048671</td>\n",
       "      <td>-0.038404</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044432</td>\n",
       "      <td>-0.007842</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>0.243409</td>\n",
       "      <td>0.154859</td>\n",
       "      <td>0.128655</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.106768</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051570</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.155760</td>\n",
       "      <td>0.048347</td>\n",
       "      <td>-0.031283</td>\n",
       "      <td>0.133782</td>\n",
       "      <td>0.085081</td>\n",
       "      <td>-0.047716</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0 -0.011858 -0.061636  0.054392  0.021137  0.089374  0.101825 -0.044525   \n",
       "1 -0.018183 -0.045645  0.002001  0.066467 -0.118108  0.105399  0.004705   \n",
       "2  0.074145 -0.044487  0.048191 -0.006614  0.042746  0.025960  0.021290   \n",
       "3  0.044432 -0.007842  0.018440  0.036639  0.243409  0.154859  0.128655   \n",
       "4  0.051570 -0.016571  0.014702  0.024296  0.155760  0.048347 -0.031283   \n",
       "\n",
       "          8         9        10  ...  labels_4  labels_5  labels_6  labels_7  \\\n",
       "0  0.222330  0.015575 -0.053591  ...        -1        -1         1         1   \n",
       "1  0.046744 -0.060929  0.284897  ...         1        -1        -1        -1   \n",
       "2  0.041714  0.048671 -0.038404  ...         1         1         1         1   \n",
       "3  0.122329  0.004179  0.106768  ...        -1        -1        -1        -1   \n",
       "4  0.133782  0.085081 -0.047716  ...        -1        -1        -1        -1   \n",
       "\n",
       "   labels_8  labels_9  labels_10  labels_11  labels_12  labels_13  \n",
       "0        -1        -1         -1          1          1         -1  \n",
       "1        -1        -1         -1          1          1         -1  \n",
       "2        -1        -1          1          1          1         -1  \n",
       "3        -1         1          1         -1         -1         -1  \n",
       "4        -1        -1         -1          1          1         -1  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "EjRcHYcJPKAf",
    "outputId": "da764cc2-3e71-4f31-8ac1-6fb525041a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k value is : 20 \n",
      "***********************************\n",
      "\n",
      "when seed is : 24 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 24 is: 0.2918510563510565 accuracy is: 0.46319179894179896\n",
      "when seed is : 34 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 34 is: 0.2900052355052357 accuracy is: 0.45811904761904754\n",
      "when seed is : 44 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 44 is: 0.28829791504791513 accuracy is: 0.45843121693121675\n",
      "when seed is : 54 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 54 is: 0.3015544363044364 accuracy is: 0.48222089947089947\n",
      "when seed is : 65 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 65 is: 0.2973836903836905 accuracy is: 0.4840925925925927\n",
      "time taken for k=20 value: 98.2779496551957\n",
      "mean f1 for k=20: 0.2938184667184668 mean accuracy is: 0.469211111111111\n",
      "k value is : 40 \n",
      "***********************************\n",
      "\n",
      "when seed is : 24 \n",
      "***************************************\n",
      "\n",
      "F1 score for test data with seed value: 24 is: 0.30638541088541105 accuracy is: 0.4965705868205871\n",
      "when seed is : 34 \n",
      "***************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-9a797a8a955c>:26: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  alpha=(1/2)*np.log((1-epsilon)/epsilon)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input sample_weight contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-a07b7a5aae0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_k_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_k_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultitask_adaboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-efd825c1bae5>\u001b[0m in \u001b[0;36mmultitask_adaboost\u001b[0;34m(data, k_values)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdaboost_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# prediction over test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdaBoost_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-9a797a8a955c>\u001b[0m in \u001b[0;36mAdaboost_fit\u001b[0;34m(data_X, data_y, rounds)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mround\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mweak_classifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mweak_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweak_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOUBLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_sample_weight\u001b[0;34m(sample_weight, X, dtype, copy, only_non_negative)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         sample_weight = check_array(\n\u001b[0m\u001b[1;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input sample_weight contains NaN."
     ]
    }
   ],
   "source": [
    "k_values=[20,40,60,80,100]\n",
    "best_k_f1,best_k_acc,f1_score_list,accuracy_list=multitask_adaboost(data,k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZ2CYjTi--ZE"
   },
   "outputs": [],
   "source": [
    "best_k,k,f1_score_cv,accuracy_cv=best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tly9xbujFnSn"
   },
   "outputs": [],
   "source": [
    "best_k=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "iz3VeRSEUO8E"
   },
   "outputs": [],
   "source": [
    "def multitask_Adaboost_training_model(data,k=90):\n",
    "  data_X=data[data.columns[:-15]]\n",
    "  data_y=data[data.columns[-15:]]\n",
    "  #Here we use complete data  as a training data\n",
    "  alphas_labels=[] # alpha vectors corresponding each labels\n",
    "  classifiers_labels=[] # classifiers corresponding each labels\n",
    "  for i in range(len(labels_arr)):\n",
    "    y=data_y[f'labels_{i}']\n",
    "\n",
    "    alphas,weights,epsilons,classifiers=Adaboost_fit(data_X,y,k)\n",
    "    # prediction over train data\n",
    "    y_pred=AdaBoost_pred(data_X,alphas,classifiers)\n",
    "    accuracy=Accuracy_adaboost(y_pred,np.array(y))\n",
    "    print(f'on train data with label{i}',accuracy)\n",
    "    # storing the alphas and classifiers\n",
    "    alphas_labels.append(alphas)\n",
    "    classifiers_labels.append(classifiers)\n",
    "  return alphas_labels,classifiers_labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f86zAq55XH-h",
    "outputId": "cf76a90f-0859-48c8-8038-af56980a1125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on train data with label0 1.0\n",
      "on train data with label1 1.0\n",
      "on train data with label2 1.0\n",
      "on train data with label3 1.0\n",
      "on train data with label4 1.0\n",
      "on train data with label5 1.0\n",
      "on train data with label6 1.0\n",
      "on train data with label7 1.0\n",
      "on train data with label8 1.0\n",
      "on train data with label9 1.0\n",
      "on train data with label10 1.0\n",
      "on train data with label11 1.0\n",
      "on train data with label12 1.0\n",
      "on train data with label13 1.0\n"
     ]
    }
   ],
   "source": [
    "alphas_labels,classifiers_labels=multitask_Adaboost_training_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "uFILZYHKXBIJ"
   },
   "outputs": [],
   "source": [
    "def multitask_Adaboost_test_model(data,alphas=alphas_labels,classifiers=classifiers_labels):\n",
    "  data_X=data[data.columns[:-15]]\n",
    "  data_y=data[data.columns[-15:]]\n",
    "  pred=[set([]) for i in range(len(data_X))]\n",
    "  for i in range(len(labels_arr)):\n",
    "    alphas_lab=alphas[i]\n",
    "    classifiers_lab=classifiers[i]\n",
    "    y_pred=AdaBoost_pred(data_X,alphas_lab,classifiers_lab)\n",
    "    print('individual accuracy :',Accuracy_adaboost(y_pred,data_y[f'labels_{i}']))\n",
    "    for m in range(len(y_pred)):\n",
    "      if y_pred[m]==1:\n",
    "        \n",
    "        pred[m]=pred[m].union({str(i)})\n",
    "  return pred      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQWfCEgzY-Rx",
    "outputId": "e98a2488-b979-4d64-d9a7-77d03b6d40ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "individual accuracy : 1.0\n",
      "accuracy is: 1.0\n",
      "f1 score is: 0.5\n"
     ]
    }
   ],
   "source": [
    "predict=multitask_Adaboost_test_model(data)\n",
    "actual=np.array(data[data.columns[-15]])\n",
    "print('accuracy is:',accuracy(predict,actual))\n",
    "print('f1 score is:',F1_score(predict,actual))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "xFBSrUWe--ZF"
   },
   "outputs": [],
   "source": [
    "with open('sample_multilabel_test_data.txt') as f:\n",
    "    data_list = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AXlE6PS--ZF",
    "outputId": "6d678dd4-3f93-48f6-a5f7-9e627f11bf14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features are: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103]\n",
      "labels are: [11  1  6 10 12 13  5  2  7  0  4  8  3  9]\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "features=[]\n",
    "\n",
    "for line in data_list:\n",
    "  line_list=line.split(' ')\n",
    "  labels.append(set(line_list[0].split(',')))\n",
    "  for j in line_list[1:-1]:\n",
    "    features.append(int(j.split(':')[0]))\n",
    "    \n",
    "features=np.array(features)\n",
    "label_set=set([])\n",
    "for i in labels:\n",
    "  label_set=set.union(label_set,i)\n",
    "label_arr=list(label_set)\n",
    "labels_arr=np.array([int(label_arr[i]) for i in range(len(label_arr))])\n",
    "\n",
    "features_arr=np.unique(features)\n",
    "print('features are:',features_arr)\n",
    "print('labels are:',labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86NJluVJ--ZF",
    "outputId": "0a2780d2-e557-4c32-bfd7-755783b058c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 50\n"
     ]
    }
   ],
   "source": [
    "# number of data points\n",
    "n=len(data_list)\n",
    "print('number of data points:',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qts7R99E--ZF",
    "outputId": "4a7f0680-4d27-4131-9cf7-b0438b32c64f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 103\n"
     ]
    }
   ],
   "source": [
    "#number of features\n",
    "d=len(features_arr)\n",
    "print('number of features:',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dspZ9sNI--ZG",
    "outputId": "2a05b25a-ab07-45d6-9315-a2470a38082c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 103)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array=np.array([0.0 for i in range(n*(d))]).reshape(n,d)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yZDTVim--ZG",
    "outputId": "67ea4237-14cc-4d55-a7ec-eea904950d5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for i in range(len(data_list)):\n",
    "  line=data_list[i]\n",
    "  line_list=line.split(' ')\n",
    "  labels.append(set(line_list[0].split(',')))\n",
    "  #print(i)\n",
    "  for j in line_list[1:-1]:\n",
    "    a=j.split(':')\n",
    "    #print(a,int(a[0])-1,a[1])\n",
    "    array[i,int(a[0])-1]=a[1]\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "-O1V2SLf--ZG"
   },
   "outputs": [],
   "source": [
    "labels_arr.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "NAlmZR-f--ZG"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(array,columns=features_arr)\n",
    "df['labels']=labels\n",
    "for i in labels_arr:\n",
    "  df[f'labels_{i}']=df['labels'].apply(lambda x: lab(x,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "EgKPyI6P--ZG",
    "outputId": "abf2aa29-889e-4aa3-c9c6-f2e9f1d073ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>labels_4</th>\n",
       "      <th>labels_5</th>\n",
       "      <th>labels_6</th>\n",
       "      <th>labels_7</th>\n",
       "      <th>labels_8</th>\n",
       "      <th>labels_9</th>\n",
       "      <th>labels_10</th>\n",
       "      <th>labels_11</th>\n",
       "      <th>labels_12</th>\n",
       "      <th>labels_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097750</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.115902</td>\n",
       "      <td>0.075859</td>\n",
       "      <td>0.097757</td>\n",
       "      <td>0.214842</td>\n",
       "      <td>0.215689</td>\n",
       "      <td>0.177362</td>\n",
       "      <td>0.178295</td>\n",
       "      <td>0.116535</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017362</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>0.071415</td>\n",
       "      <td>0.080824</td>\n",
       "      <td>-0.158254</td>\n",
       "      <td>-0.069842</td>\n",
       "      <td>-0.165107</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0.142120</td>\n",
       "      <td>0.042755</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007123</td>\n",
       "      <td>-0.118310</td>\n",
       "      <td>0.134876</td>\n",
       "      <td>-0.004037</td>\n",
       "      <td>0.132463</td>\n",
       "      <td>0.126321</td>\n",
       "      <td>0.117789</td>\n",
       "      <td>-0.043311</td>\n",
       "      <td>-0.002460</td>\n",
       "      <td>-0.014554</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.157322</td>\n",
       "      <td>-0.066659</td>\n",
       "      <td>-0.191130</td>\n",
       "      <td>-0.097811</td>\n",
       "      <td>-0.223990</td>\n",
       "      <td>-0.053608</td>\n",
       "      <td>-0.170721</td>\n",
       "      <td>-0.134466</td>\n",
       "      <td>-0.069921</td>\n",
       "      <td>0.099598</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.159182</td>\n",
       "      <td>0.299849</td>\n",
       "      <td>0.116136</td>\n",
       "      <td>0.076483</td>\n",
       "      <td>-0.046788</td>\n",
       "      <td>0.076437</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.097750  0.059746  0.115902  0.075859  0.097757  0.214842  0.215689   \n",
       "1  0.017362 -0.004442  0.071415  0.080824 -0.158254 -0.069842 -0.165107   \n",
       "2 -0.007123 -0.118310  0.134876 -0.004037  0.132463  0.126321  0.117789   \n",
       "3 -0.157322 -0.066659 -0.191130 -0.097811 -0.223990 -0.053608 -0.170721   \n",
       "4  0.063291  0.159182  0.299849  0.116136  0.076483 -0.046788  0.076437   \n",
       "\n",
       "          8         9        10  ...  labels_4  labels_5  labels_6  labels_7  \\\n",
       "0  0.177362  0.178295  0.116535  ...        -1        -1        -1        -1   \n",
       "1 -0.029324  0.142120  0.042755  ...         1        -1        -1        -1   \n",
       "2 -0.043311 -0.002460 -0.014554  ...         1         1        -1        -1   \n",
       "3 -0.134466 -0.069921  0.099598  ...        -1        -1        -1        -1   \n",
       "4  0.003833  0.003026  0.022628  ...        -1        -1        -1        -1   \n",
       "\n",
       "   labels_8  labels_9  labels_10  labels_11  labels_12  labels_13  \n",
       "0        -1        -1         -1          1          1         -1  \n",
       "1        -1        -1         -1          1          1         -1  \n",
       "2        -1        -1         -1         -1         -1         -1  \n",
       "3        -1         1          1          1          1         -1  \n",
       "4        -1        -1         -1          1          1         -1  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "L82HatC_--ZG"
   },
   "outputs": [],
   "source": [
    "test_data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZxb0K7d--ZG",
    "outputId": "4dce3b3d-ab14-471a-f24f-9fd0a09a89b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "individual accuracy : 0.84\n",
      "individual accuracy : 0.6\n",
      "individual accuracy : 0.7\n",
      "individual accuracy : 0.7\n",
      "individual accuracy : 0.7\n",
      "individual accuracy : 0.64\n",
      "individual accuracy : 0.84\n",
      "individual accuracy : 0.86\n",
      "individual accuracy : 0.98\n",
      "individual accuracy : 0.86\n",
      "individual accuracy : 0.82\n",
      "individual accuracy : 0.68\n",
      "individual accuracy : 0.64\n",
      "individual accuracy : 0.98\n",
      "accuracy is: 0.32639682539682546\n",
      "f1 score is: 0.22597868797868795\n"
     ]
    }
   ],
   "source": [
    "pred_test=multitask_Adaboost_test_model(test_data)\n",
    "actual=np.array(test_data[test_data.columns[-15]])\n",
    "print('accuracy is:',accuracy(predict,actual))\n",
    "print('f1 score is:',F1_score(predict,actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qemmy5B4Iw0r",
    "outputId": "a4b02ca7-a1a1-4f22-ad5f-f2585a230f68"
   },
   "outputs": [],
   "source": [
    "d={'pred':pred_test,'actual':actual}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "MnR_0ZmOJU9a"
   },
   "outputs": [],
   "source": [
    "with open('my_data.pkl', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_data.pkl', 'rb') as f:\n",
    "    my_function = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_function['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(multitask_Adaboost_training_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as f:\n",
    "    my_function = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.multitask_Adaboost_training_model(data, k=90)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " my_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
